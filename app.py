# --- START OF FILE app.py ---

import streamlit as st
import os
import numpy as np
import librosa
import soundfile as sf
from pydub import AudioSegment, silence
import nltk
from nltk.tokenize import sent_tokenize
from TTS.api import TTS
import google.generativeai as genai
from transformers import pipeline
import logging
from collections import Counter
import torch
import torch.nn.functional as F
from laion_clap import CLAP_Module # Import CLAP here as well
import time # For simulating work/pauses if needed, and unique filenames
import traceback # For detailed error logging

# --- Streamlit Page Setup ---
st.set_page_config(page_title="AI Audio Story Generator", layout="wide")
st.title("üéôÔ∏è AI Audio Story Generator with Background Sounds")

# --- Configuration ---
# Using session state to keep track of generated files for potential cleanup
if 'output_audio_path' not in st.session_state:
    st.session_state.output_audio_path = None
if 'temp_dir' not in st.session_state:
    # Create a unique temp dir for each run potentially, or manage cleanup
    st.session_state.temp_dir = f"temp_audio_st_{int(time.time())}"

SOUND_DIR = "trimmed_sounds"                # Directory with background sounds (.wav)
AUDIO_EMBEDDINGS_PATH = "audio_embeddings.pt" # Generated by embed_audio.py
TEMP_AUDIO_DIR = st.session_state.temp_dir      # Temporary directory for TTS files
FINAL_OUTPUT_FILENAME_BASE = "final_story_output" # Base name for output

# --- Sidebar for Parameters ---
st.sidebar.header("Configuration")
similarity_threshold = st.sidebar.slider(
    "Background Sound Similarity Threshold", 0.0, 1.0, 0.25, 0.01,
    help="How closely text must match sound embedding (higher means stricter match)."
)
background_db_reduction = st.sidebar.slider(
    "Background Sound Volume Reduction (dB)", -30, 0, -12, 1,
    help="How much quieter background sounds are relative to speech (more negative = quieter)."
)
pause_duration_ms = st.sidebar.slider(
    "Pause Between Sentences (ms)", 0, 1500, 400, 50,
    help="Duration of silence added between sentences."
)

# --- Model Loading (Cached) ---

# Setup basic logging for debugging in console
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Ensure required NLTK data is downloaded (run once)
@st.cache_data # Cache the fact that download was attempted/done
def download_nltk_punkt():
    try:
        nltk.data.find('tokenizers/punkt')
        logger.info("NLTK 'punkt' tokenizer already available.")
    except LookupError:
        st.info("Downloading NLTK 'punkt' tokenizer (one-time setup)...")
        logger.info("Downloading NLTK 'punkt' tokenizer...")
        nltk.download('punkt', quiet=True)
        logger.info("NLTK 'punkt' downloaded.")
        st.success("Tokenizer downloaded.")
download_nltk_punkt()

# --- Cached Model Initializers ---

@st.cache_resource # Caches the actual model object
def load_tts_model():
    try:
        # *** USING XTTS v2 ***
        model_name = "tts_models/multilingual/multi-dataset/xtts_v2"
        logger.info(f"Initializing TTS model ({model_name})...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tts_model = TTS(model_name).to(device)
        logger.info(f"XTTS model initialized and moved to target device: {device}")
        return tts_model
    except Exception as e:
        st.error(f"Fatal Error initializing TTS model: {e}")
        logger.error(f"Error initializing TTS ({model_name}): {e}", exc_info=True)
        return None

@st.cache_resource
def load_emotion_classifiers():
    classifiers = {}
    logger.info("Initializing emotion classifiers...")
    models = {
        "primary": "j-hartmann/emotion-english-distilroberta-base",
        "secondary": "bhadresh-savani/distilbert-base-uncased-emotion"
    }
    device_id = 0 if torch.cuda.is_available() else -1 # Use -1 for CPU with transformers
    for name, model_name in models.items():
        try:
            logger.info(f"Loading classifier {name}: {model_name}")
            classifiers[name] = pipeline(
                "text-classification",
                model=model_name,
                top_k=None, # Return all classes + scores
                device=device_id
            )
            logger.info(f"Classifier {name} loaded successfully.")
        except Exception as e:
            st.warning(f"Could not load emotion classifier {name} ({model_name}): {e}")
            logger.error(f"Error initializing emotion classifier {name} ({model_name}): {e}")

    if not classifiers: logger.warning("No emotion classifiers could be initialized.")
    else: logger.info(f"Initialized {len(classifiers)} emotion classifier(s).")
    return classifiers if classifiers else None # Return None if empty

@st.cache_resource
def load_clap_model_and_embeddings():
    clap_model_instance = None
    audio_embeddings_data = None
    clap_device = "cuda" if torch.cuda.is_available() else "cpu"

    if not os.path.exists(AUDIO_EMBEDDINGS_PATH):
        st.warning(f"Audio embeddings file not found: {AUDIO_EMBEDDINGS_PATH}. Background sounds disabled.")
        return None, None

    try:
        logger.info("Initializing CLAP model...")
        clap_model_instance = CLAP_Module(enable_fusion=False).to(clap_device)
        ckpt_path = os.path.expanduser("~/.cache/clap/630k-audioset-best.pt")

        if os.path.exists(ckpt_path):
            logger.info("Loading CLAP checkpoint...")
            try:
                # This relies on the previous fix (modifying library or add_safe_globals)
                # If this fails, clap_model_instance remains None below
                clap_model_instance.load_ckpt(ckpt=ckpt_path)
                logger.info("CLAP checkpoint loaded successfully.")
                clap_model_instance.eval()
            except Exception as clap_load_e:
                st.error(f"Failed to load CLAP checkpoint: {clap_load_e}. Background sounds disabled. Check library modification or safe globals.")
                logger.error(f"Failed to load CLAP checkpoint: {clap_load_e}", exc_info=True)
                clap_model_instance = None # Crucial: disable CLAP if loading fails
        else:
             st.warning(f"CLAP checkpoint not found at {ckpt_path}. Background sounds disabled.")
             clap_model_instance = None # Disable CLAP if no checkpoint

        # Load embeddings only if CLAP model loaded successfully
        if clap_model_instance:
            logger.info(f"Loading audio embeddings from {AUDIO_EMBEDDINGS_PATH}...")
            # Load embeddings to CPU first, then move to device during processing if needed
            audio_embeddings_data = torch.load(AUDIO_EMBEDDINGS_PATH, map_location='cpu')
            logger.info(f"Loaded {len(audio_embeddings_data)} audio embeddings.")
        else:
             audio_embeddings_data = None # Ensure embeddings are None if CLAP failed

    except Exception as e:
        st.error(f"Error during CLAP/Embedding initialization: {e}. Background sounds disabled.")
        logger.error(f"Failed during CLAP/Embedding initialization: {e}", exc_info=True)
        clap_model_instance = None
        audio_embeddings_data = None

    return clap_model_instance, audio_embeddings_data


# --- Core Logic Functions (Adapted from modulation_2.py) ---

# Note: These functions are kept similar to your original script but adapted
# slightly for Streamlit integration (e.g., logging, file paths).

def generate_story(prompt, status_placeholder):
    """Generate a story using Google's Gemini API"""
    api_key_env = os.environ.get("GOOGLE_API_KEY")
    if not api_key_env:
        status_placeholder.warning("GOOGLE_API_KEY not found in environment. Using fallback story.")
        logger.warning("No Google API key found in environment. Using fallback story.")
        # Fallback story
        return """
        Pip, a small field mouse with a bright pink nose, tiptoed into the whispering woods. Crunch, crunch, crunch went his tiny paws on the fallen autumn leaves. Chirp, chirp, chirp sang the robins hidden amongst the branches. Pip stopped, his whiskers twitching. A distant Hoo-hoo echoed through the trees. A little scared, Pip peeked around a giant oak. The leaves rustled gently in the breeze like a soft whisper. Suddenly, Pip gasped! A beautiful clearing filled with glowing fireflies blinked before him. It was magical! He stood very still, filled with quiet wonder, his little heart thumping with happiness. The forest felt like a secret, just for him.
        """
    try:
        genai.configure(api_key=api_key_env)
        model = genai.GenerativeModel("gemini-1.5-pro-latest") # Or "gemini-pro"
        status_placeholder.write("Generating story content via Gemini API...")
        logger.info("Generating story content...")
        response = model.generate_content(prompt)
        logger.info("Story content received from API.")

        # Safely extract text
        if hasattr(response, 'parts') and response.parts:
             if hasattr(response.parts[0], 'text'):
                  return response.parts[0].text
             else: raise ValueError("Generation response format unexpected (no text)")
        elif hasattr(response, 'text'): return response.text
        else:
             feedback_info = f" Reason: {response.prompt_feedback}" if hasattr(response, 'prompt_feedback') else ""
             raise ValueError(f"Generation failed or blocked.{feedback_info}")

    except Exception as e:
        status_placeholder.error(f"Error generating story: {e}. Using fallback.")
        logger.error(f"Error generating story: {e}. Using fallback.")
        return """
        (Fallback Story due to error) Pip, a small field mouse with a bright pink nose, tiptoed into the whispering woods. Crunch, crunch, crunch went his tiny paws on the fallen autumn leaves. Chirp, chirp, chirp sang the robins hidden amongst the branches. Pip stopped, his whiskers twitching. A distant Hoo-hoo echoed through the trees. A little scared, Pip peeked around a giant oak. The leaves rustled gently in the breeze like a soft whisper. Suddenly, Pip gasped! A beautiful clearing filled with glowing fireflies blinked before him. It was magical! He stood very still, filled with quiet wonder, his little heart thumping with happiness. The forest felt like a secret, just for him.
        """

# Emotion detection functions (rule_based_emotion, detect_emotion) remain the same as in modulation_2.py
# Copy them here directly or import if you keep modulation_2.py as a module
def rule_based_emotion(sentence):
    text = sentence.lower()
    happy_words = ["happy", "joy", "excited", "laugh", "smile", "wonderful", "amazing", "delighted", "thrilled", "glee", "cheerful", "overjoyed", "enthusiastic", "best picnic ever", "magical", "glowing fireflies", "happiness"]
    if any(word in text for word in happy_words): return "happy"
    sad_words = ["sad", "cry", "tears", "unhappy", "miserable", "unfortunate", "sorry", "depressed", "gloomy", "heartbroken", "disappointed", "upset", "wept", "ruined", "sadly"]
    if any(word in text for word in sad_words): return "sad"
    angry_words = ["angry", "mad", "furious", "yell", "shout", "rage", "irritated", "frustrated", "enraged", "annoyed", "hate", "temper", "outraged", "how dare you"]
    if any(word in text for word in angry_words): return "angry"
    fear_words = ["scared", "afraid", "terrified", "fear", "frightened", "panic", "worried", "anxious", "dread", "horrified", "trembling", "hid under", "a little scared"]
    if any(word in text for word in fear_words): return "sad" # Map fear to sad
    return None

def detect_emotion(classifiers, sentence):
    rule_emotion = rule_based_emotion(sentence)
    if rule_emotion: return rule_emotion
    if not classifiers: return "neutral"
    detected_emotions = []
    confidence_threshold = 0.4
    for name, classifier in classifiers.items():
        try:
            result = classifier(sentence)[0] # pipeline returns list of lists
            top_pred = max(result, key=lambda x: x['score']) # find highest score dict in list
            emotion = top_pred["label"].lower()
            confidence = top_pred["score"]
            if confidence >= confidence_threshold: detected_emotions.append(emotion)
        except Exception as e: logger.error(f"Error with classifier {name}: {e}")
    if not detected_emotions: return "neutral"
    emotion_mapping = {"joy": "happy", "happiness": "happy", "sadness": "sad", "anger": "angry", "fear": "sad", "surprise": "happy", "disgust": "angry", "neutral": "neutral", "love": "happy", "happy": "happy", "sad": "sad", "angry": "angry"}
    mapped_emotions = [emotion_mapping.get(e, "neutral") for e in detected_emotions]
    emotion_counts = Counter(mapped_emotions)
    final_emotion = emotion_counts.most_common(1)[0][0]
    return final_emotion

def apply_modulation(tts, text, emotion, reference_wav_path, output_path, status_placeholder):
    """Apply subtle energy modulation using XTTSv2."""
    try:
        modulation = {
            "happy": {"energy": 1.05}, "sad": {"energy": 0.95},
            "angry": {"energy": 1.0}, "neutral": {"energy": 1.0},
        }
        params = modulation.get(emotion, modulation["neutral"])

        if not tts: raise ValueError("TTS model not initialized")
        if not os.path.exists(reference_wav_path): raise FileNotFoundError(f"Reference WAV not found: {reference_wav_path}")

        # XTTS call
        tts.tts_to_file(
            text=text, speaker_wav=reference_wav_path, language="en", file_path=output_path
        )

        # Post-processing: ONLY subtle energy/volume adjustment
        if os.path.exists(output_path):
            try:
                audio, sr = librosa.load(output_path, sr=None)
                audio = audio * params["energy"]
                # Normalize
                max_amp = np.max(np.abs(audio))
                if max_amp > 0.98: audio = audio * (0.98 / max_amp)
                sf.write(output_path, audio, sr)
                return output_path
            except Exception as post_proc_e:
                status_placeholder.warning(f"Modulation post-processing failed: {post_proc_e}. Using original TTS.")
                logger.error(f"Error during post-processing for {output_path}: {post_proc_e}")
                return output_path # Return original if post-processing fails
        else:
            raise RuntimeError(f"TTS failed to create file: {output_path}")

    except Exception as e:
        status_placeholder.error(f"Error during TTS/Modulation for '{text[:30]}...': {e}")
        logger.error(f"Error applying modulation: {e}", exc_info=True)
        # Attempt fallback neutral TTS
        try:
            if tts and os.path.exists(reference_wav_path):
                 status_placeholder.warning("Attempting fallback neutral TTS...")
                 tts.tts_to_file(text=text, speaker_wav=reference_wav_path, language="en", file_path=output_path)
                 if os.path.exists(output_path): return output_path
            return None # Fallback failed or preconditions not met
        except Exception as fb_e:
             status_placeholder.error(f"Fallback TTS also failed: {fb_e}")
             return None
        
def process_audio_generation(
    tts, classifiers, clap_model, narration_text, reference_wav_path,
    audio_embeddings, sound_dir, output_dir, final_output_path_base,
    similarity_thr, bg_db_reduction, pause_ms, status_placeholder, progress_bar
    ):
    """Main processing loop adapted for Streamlit status updates."""

    if audio_embeddings and not clap_model:
         status_placeholder.error("CLAP model needed for background sound matching but failed to load.")
         return None # Cannot proceed with sound matching

    try:
        os.makedirs(output_dir, exist_ok=True)
        narration_text = narration_text.replace('\x00', '').strip()
        sentences = sent_tokenize(narration_text)
        num_sentences = len(sentences)
        if num_sentences == 0:
             status_placeholder.error("Narration text resulted in zero sentences.")
             return None

        status_placeholder.write(f"Processing {num_sentences} sentences...")

        processed_segments = []
        emotion_summary = Counter()
        sound_match_summary = Counter()
        clap_device = "cpu"
        if clap_model:
            try: clap_device = next(clap_model.parameters()).device
            except: status_placeholder.warning("Could not get CLAP device, using CPU.")

        # --- Sentence Processing Loop ---
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                progress_bar.progress((i + 1) / num_sentences)
                continue

            status_placeholder.write(f"Processing Sentence {i+1}/{num_sentences}: '{sentence[:50]}...'")
            logger.info(f"Sentence {i+1}/{num_sentences}: {sentence}")

            # Emotion Detection
            emotion = detect_emotion(classifiers, sentence)
            emotion_summary[emotion] += 1
            status_placeholder.write(f"  Detected Emotion: {emotion}")
            logger.info(f"  Emotion: {emotion}")

            # TTS Modulation
            temp_tts_path = os.path.join(output_dir, f"sentence_{i:03d}_{emotion}_tts.wav")
            modulated_tts_path = apply_modulation(tts, sentence, emotion, reference_wav_path, temp_tts_path, status_placeholder)

            if not modulated_tts_path or not os.path.exists(modulated_tts_path):
                 status_placeholder.warning(f"  Skipping sentence {i+1} due to TTS/Modulation failure.")
                 progress_bar.progress((i + 1) / num_sentences)
                 continue

            try:
                 tts_segment = AudioSegment.from_wav(modulated_tts_path)
            except Exception as e:
                 status_placeholder.error(f"  Error loading TTS segment: {e}. Skipping sentence.")
                 progress_bar.progress((i + 1) / num_sentences)
                 continue

            # Background Sound Matching
            best_match_sound = None
            if clap_model and audio_embeddings:
                try:
                    with torch.no_grad():
                        text_embedding = clap_model.get_text_embedding([sentence], use_tensor=True)
                        if text_embedding is None: raise ValueError("Text embedding failed")
                        text_embedding = text_embedding.to(clap_device)

                        highest_similarity = -1
                        for sound_fname, audio_embedding in audio_embeddings.items():
                            audio_embedding_dev = audio_embedding.to(clap_device)
                            similarity = F.cosine_similarity(text_embedding, audio_embedding_dev, dim=1).item()
                            if similarity > highest_similarity:
                                highest_similarity = similarity
                                best_match_sound = sound_fname

                    if best_match_sound and highest_similarity >= similarity_thr:
                        status_placeholder.write(f"  Best sound match: '{best_match_sound}' (Sim: {highest_similarity:.3f})")
                        sound_match_summary[best_match_sound] += 1
                    else:
                        status_placeholder.write(f"  No sound match above threshold {similarity_thr:.2f}.")
                        best_match_sound = None
                except Exception as e:
                    status_placeholder.warning(f"  Error during sound matching: {e}")
                    logger.error(f"  Error during background sound matching: {e}", exc_info=True)
                    best_match_sound = None

            # Mixing
            final_segment = tts_segment
            if best_match_sound:
                try:
                    bg_sound_path = os.path.join(sound_dir, best_match_sound)
                    if not os.path.exists(bg_sound_path):
                         status_placeholder.warning(f"  Sound file not found: {bg_sound_path}")
                    else:
                        bg_segment = AudioSegment.from_wav(bg_sound_path)
                        bg_segment = bg_segment + bg_db_reduction # Reduce volume

                        tts_duration_ms = len(tts_segment)
                        bg_duration_ms = len(bg_segment)
                        if bg_duration_ms == 0: pass # Skip zero duration sounds
                        elif bg_duration_ms < tts_duration_ms:
                            repeat_count = int(np.ceil(tts_duration_ms / bg_duration_ms))
                            bg_segment = (bg_segment * repeat_count)[:tts_duration_ms]
                        else: bg_segment = bg_segment[:tts_duration_ms]

                        if bg_segment and len(bg_segment) > 0:
                            final_segment = tts_segment.overlay(bg_segment)
                            status_placeholder.write(f"  Mixed with '{best_match_sound}' ({bg_db_reduction} dB).")
                        else:
                             status_placeholder.warning(f"  Background segment for {best_match_sound} resulted in zero length after processing.")

                except Exception as e:
                    status_placeholder.warning(f"  Error mixing sound {best_match_sound}: {e}")
                    logger.error(f"  Error mixing background sound {best_match_sound}: {e}")

            processed_segments.append(final_segment)

            # Add Pause
            if pause_ms > 0:
                processed_segments.append(AudioSegment.silent(duration=pause_ms))

            # Update Progress Bar
            progress_bar.progress((i + 1) / num_sentences)
            # End of sentence loop

        status_placeholder.write("Sentence processing complete. Merging audio segments...")
        if not processed_segments:
            status_placeholder.error("No audio segments were processed.")
            return None

        # Remove final pause if one was added
        if len(processed_segments) > 1 and processed_segments[-1].duration_seconds * 1000 == pause_ms and len(processed_segments[-1].get_array_of_samples()) == 0 :
             processed_segments.pop()

        valid_segments = [seg for seg in processed_segments if isinstance(seg, AudioSegment)]
        if not valid_segments:
            status_placeholder.error("No valid audio segments to merge.")
            return None

        combined_audio = sum(valid_segments)

        # Ensure final output dir exists
        final_output_dir = os.path.dirname(final_output_path_base)
        if final_output_dir and not os.path.exists(final_output_dir):
            os.makedirs(final_output_dir)

        # Create a unique filename for each run
        timestamp = int(time.time())
        final_output_path = f"{final_output_path_base}_{timestamp}.wav"

        status_placeholder.write(f"Exporting final audio to {final_output_path}...")
        combined_audio.export(final_output_path, format="wav")
        logger.info(f"Final audio saved to {final_output_path} - Duration: {len(combined_audio)/1000:.2f}s")

        # Log summaries
        logger.info("-" * 20)
        logger.info("Processing Summary:")
        logger.info(f"  Sentences processed: {num_sentences}")
        logger.info("  Emotion Counts:")
        for emotion, count in emotion_summary.items(): logger.info(f"    {emotion}: {count}")
        logger.info("  Background Sound Matches:")
        if sound_match_summary:
            for sound, count in sound_match_summary.items(): logger.info(f"    {sound}: {count}")
        else: logger.info("    No background sounds were matched or used.")

        return final_output_path # Return the path to the final audio file

    except Exception as e:
        status_placeholder.error(f"Critical error during audio generation: {e}")
        logger.error(f"Critical error in process_audio_generation: {e}", exc_info=True)
        st.error(f"An unexpected error occurred: {e}\n{traceback.format_exc()}") # Show full traceback in Streamlit
        return None

# --- Streamlit UI Elements ---

st.header("1. Input Story Prompt")
prompt_text = st.text_area("Enter the text you want to convert to an audio story:", height=150, placeholder="e.g., Once upon a time, in a land filled with chirping birds...")

st.header("2. Upload Reference Voice")
uploaded_file = st.file_uploader("Choose a short (~10-30s) WAV file of the desired voice:", type=['wav'])

# Placeholder for displaying the generated story text
story_display = st.empty()
# Placeholder for the final audio player
audio_player = st.empty()

st.header("3. Generate Audio Story")
generate_button = st.button("‚ú® Generate Story Audio ‚ú®")

if generate_button:
    if not prompt_text:
        st.warning("Please enter a story prompt.")
    elif uploaded_file is None:
        st.warning("Please upload a reference WAV file.")
    else:
        # --- Start Processing ---
        # Clear previous results
        story_display.empty()
        audio_player.empty()
        st.session_state.output_audio_path = None

        # Save uploaded file temporarily
        reference_wav_path = os.path.join(TEMP_AUDIO_DIR, f"reference_{uploaded_file.name}")
        os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)
        with open(reference_wav_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.info(f"Reference voice saved temporarily to: {reference_wav_path}")

        # Initialize models (will use cache if already loaded)
        with st.spinner("Loading AI models (TTS, Emotion, CLAP)... Please wait."):
            tts_model = load_tts_model()
            emotion_classifiers = load_emotion_classifiers()
            clap_model, audio_embeddings = load_clap_model_and_embeddings()

        if not tts_model:
            st.error("TTS Model failed to load. Cannot proceed.")
        else:
            # Use st.status for dynamic updates
            with st.status("Generating audio story...", expanded=True) as status:
                try:
                    # Step 1: Generate Story Text
                    status.write("Generating story text...")
                    narration_text = generate_story(prompt_text, status)
                    if narration_text:
                        status.write("Story text generated successfully.")
                        story_display.markdown(f"**Generated Story:**\n\n```\n{narration_text}\n```")
                    else:
                        status.error("Failed to generate story text.")
                        raise Exception("Story generation failed.") # Stop processing

                    # Step 2: Process Audio
                    status.write("Starting audio processing (TTS, Emotion, Sounds)...")
                    progress_bar = st.progress(0.0) # Initialize progress bar

                    final_audio_file = process_audio_generation(
                        tts=tts_model,
                        classifiers=emotion_classifiers,
                        clap_model=clap_model,
                        narration_text=narration_text,
                        reference_wav_path=reference_wav_path,
                        audio_embeddings=audio_embeddings,
                        sound_dir=SOUND_DIR,
                        output_dir=TEMP_AUDIO_DIR,
                        final_output_path_base=FINAL_OUTPUT_FILENAME_BASE,
                        similarity_thr=similarity_threshold,
                        bg_db_reduction=background_db_reduction,
                        pause_ms=pause_duration_ms,
                        status_placeholder=status, # Pass status object for updates
                        progress_bar=progress_bar
                    )

                    if final_audio_file and os.path.exists(final_audio_file):
                        status.update(label="‚úÖ Audio Generation Complete!", state="complete", expanded=False)
                        st.session_state.output_audio_path = final_audio_file
                        # Display audio outside the status box
                    else:
                         status.update(label="‚ùå Audio Generation Failed", state="error", expanded=True)
                         st.session_state.output_audio_path = None # Ensure no stale path

                except Exception as e:
                    st.error(f"An error occurred during processing: {e}")
                    logger.error(f"Error in main processing block: {e}", exc_info=True)
                    if 'status' in locals() and status: # Check if status exists
                         status.update(label="‚ùå Processing Failed", state="error", expanded=True)
                    # Also show traceback in main area for debugging
                    st.exception(e)


            # --- Display Final Audio (if successful) ---
            if st.session_state.output_audio_path:
                st.subheader("üîä Final Audio Story")
                try:
                    with open(st.session_state.output_audio_path, 'rb') as audio_file:
                        audio_bytes = audio_file.read()
                    audio_player.audio(audio_bytes, format='audio/wav')
                    st.success(f"Successfully generated audio: {os.path.basename(st.session_state.output_audio_path)}")
                    # Optional: Add download button
                    # st.download_button(
                    #     label="Download Story Audio",
                    #     data=audio_bytes,
                    #     file_name=os.path.basename(st.session_state.output_audio_path),
                    #     mime='audio/wav'
                    # )
                except Exception as e:
                    st.error(f"Error displaying final audio: {e}")
                    logger.error(f"Error reading/displaying audio file {st.session_state.output_audio_path}: {e}")


# --- Optional: Clean up temporary files (Add button or automatic logic if needed) ---
# st.sidebar.button("Clean Temporary Files") # Example cleanup trigger
# Consider more robust cleanup if running persistently

# --- END OF FILE app.py ---